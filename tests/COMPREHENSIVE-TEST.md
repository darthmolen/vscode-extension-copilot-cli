# Comprehensive Test Suite

## Overview

The comprehensive test suite (`comprehensive-test.js`) is a complete integration test that validates the VS Code Copilot CLI Extension v2 SDK integration.

It orchestrates:
- **8 test scenarios** covering different extension capabilities
- **Event capture** of tool executions and responses
- **Automated evaluation** using the judge skill
- **Comprehensive reporting** (JSON and Markdown)

## Quick Start

### Prerequisites

1. **GitHub Copilot CLI** must be installed and authenticated:
   ```bash
   copilot --version
   ```

2. **Extension must be compiled**:
   ```bash
   npm run compile
   ```

3. **Judge skill** (optional but recommended for automated evaluation):
   - The test will use `copilot --skill judge-test-output` for evaluation
   - If judge skill is not available, manual evaluation can be done from the test outputs

### Running the Test Suite

**Easy way (using npm script):**
```bash
npm run test:comprehensive
```

**Direct way:**
```bash
node tests/comprehensive-test.js
```

## What It Tests

### Test Scenarios

1. **File Creation Test** - Multiple file operations
2. **Code Reading Test** - File reading and explanation
3. **Markdown Rendering Test** - Complex markdown content
4. **Code Fix Test** - Bug detection without modification
5. **Plan Analysis Test** - Document summarization
6. **Mixed Content Test** - Code + explanation rendering
7. **Tool Chain Test** - Sequential tool execution
8. **MCP Integration Test** - Model Context Protocol tools

### What Gets Measured

For each test:
- âœ… **Tools executed** (names, count, duration)
- ğŸ“ **Response content** (full output)
- â±ï¸ **Execution time** (per test)
- ğŸ“Š **Quality score** (0-10 from judge)
- ğŸ” **Detailed feedback** (component breakdown)

## Test Output

### Console Output

During execution, you'll see:

```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                                                          â•‘
â•‘     COPILOT CLI EXTENSION V2 - COMPREHENSIVE TEST        â•‘
â•‘                                                          â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ğŸ“¦ SETUP PHASE

âœ… Output directory: /path/to/tests/output
ğŸ”§ Initializing SDK Session Manager...
âœ… SDKSessionManager created
ğŸš€ Starting Copilot SDK session...
âœ… Session started successfully

ğŸ§ª TEST EXECUTION PHASE

Running 8 test scenarios...

[1/8] File Creation Test
============================================================
ğŸ“ Running: File Creation Test
   Tests file creation tool execution and visual feedback
============================================================
ğŸ“¤ Prompt: "Create 3 files: hello.txt with 'Hello'..."

   ğŸ”§ Tool started: create
   âœ… Tool completed: create (0.45s)
   ğŸ”§ Tool started: create
   âœ… Tool completed: create (0.38s)
   ...

âœ… Completed in 2.34s
   Tools executed: 3
   Events captured: 12

   ğŸ”§ Tools:
      âœ… create (0.45s)
      âœ… create (0.38s)
      âœ… create (0.41s)

[2/8] Code Reading Test
...

ğŸ“Š EVALUATION PHASE

ğŸ” Evaluating 8 test(s)...

Evaluating: File Creation Test...
Evaluating: Code Reading Test...
...

========================================
           TEST SUMMARY
========================================
Total Tests:   8
Passed:        7 âœ…
Failed:        1 âŒ
Errors:        0 âš ï¸
Pass Rate:     87.5%
========================================

ğŸ’¾ Saving reports...
âœ… Reports saved to:
   JSON: /path/to/tests/output/test-results-2024-01-25T12-00-00.json
   Markdown: /path/to/tests/output/test-report-2024-01-25T12-00-00.md

ğŸ“„ FINAL SUMMARY

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Total Tests:      8
Passed:           7 âœ…
Failed:           1 âŒ
Errors:           0 âš ï¸
Pass Rate:        87.5%
Average Score:    8.2/10
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

ğŸ“ Reports saved to:
   JSON:     /path/to/tests/output/test-results-2024-01-25T12-00-00.json
   Markdown: /path/to/tests/output/test-report-2024-01-25T12-00-00.md

ğŸ§¹ CLEANUP PHASE

Stopping SDK session...
âœ… Session stopped

âœ… Test suite completed successfully!
```

### Generated Reports

#### JSON Report (`test-results-*.json`)

```json
{
  "timestamp": "2024-01-25T12:00:00.000Z",
  "summary": {
    "total": 8,
    "passed": 7,
    "failed": 1,
    "errors": 0
  },
  "results": [
    {
      "testName": "File Creation Test",
      "score": 9.2,
      "status": "pass",
      "feedback": "Excellent execution...",
      "breakdown": {
        "functionality": 9.5,
        "visualization": 9.0,
        "formatting": 9.0
      }
    },
    ...
  ]
}
```

#### Markdown Report (`test-report-*.md`)

Contains:
- Summary table (pass/fail/error counts)
- Overall status
- Results table (all tests with scores)
- Detailed breakdown per test
- Component scores
- Detailed feedback from judge

See `tests/evaluation/sample-report.md` for an example.

## Exit Codes

- `0` - Success (pass rate â‰¥ 80%)
- `1` - Failure (pass rate < 80% or critical error)

## Configuration Options

### Modifying Test Behavior

Edit `comprehensive-test.js` to customize:

```javascript
const config = {
  model: 'claude-3-5-sonnet-20241022',  // Change model
  yoloMode: true,                        // Auto-approve all tools
  allowAllTools: true                    // Allow all tool types
};
```

### Adding New Scenarios

Edit `tests/scenarios.js`:

```javascript
{
  name: "My New Test",
  description: "What this test validates",
  prompt: "The prompt to send to Copilot",
  expectedTools: ["tool1", "tool2"],
  evaluationNotes: "Verify: (1) Thing 1, (2) Thing 2..."
}
```

### Adjusting Evaluation

Edit pass threshold in `tests/evaluation/criteria.js`:

```javascript
const PASS_THRESHOLD = 7.0;  // Default: 7.0/10
```

## Troubleshooting

### "copilot command not found"

Install GitHub Copilot CLI:
```bash
gh extension install github/gh-copilot
```

### "Judge skill invocation failed"

The judge skill is optional. Tests will still run, but evaluation scores may be unavailable. Check:
```bash
copilot --skill judge-test-output "test input"
```

### "SDKSessionManager not found"

Rebuild the extension:
```bash
npm run compile
```

### Session hangs or times out

- Check if `copilot` CLI is responsive: `copilot --version`
- Increase wait times in the test if needed
- Check console output for error messages

### Tests fail but manual execution works

- The test environment may differ from VS Code
- Check `vscode` mock in `comprehensive-test.js`
- Verify configuration settings match extension settings

## Development

### Running Individual Phases

You can import and use individual components:

```javascript
const scenarios = require('./scenarios');
const { evaluateTestOutput } = require('./evaluation');

// Run specific scenario
const scenario = scenarios[0];
// ... run test ...

// Evaluate specific output
const result = await evaluateTestOutput({
  name: scenario.name,
  output: "test output here",
  evaluationNotes: scenario.evaluationNotes
});
```

### Debugging

Enable detailed logging:

```javascript
class TestLogger {
  debug(...args) { console.log('[DEBUG]', ...args); } // Uncomment
}
```

Watch events in real-time:

```javascript
manager.onMessage((event) => {
  console.log('Event:', JSON.stringify(event, null, 2));
  eventCapture.captureEvent(event);
});
```

## Files Structure

```
tests/
â”œâ”€â”€ comprehensive-test.js      â† Main orchestrator
â”œâ”€â”€ scenarios.js               â† Test scenarios
â”œâ”€â”€ evaluation/
â”‚   â”œâ”€â”€ index.js              â† Evaluation framework entry
â”‚   â”œâ”€â”€ evaluator.js          â† Judge skill integration
â”‚   â”œâ”€â”€ reporter.js           â† Report generation
â”‚   â””â”€â”€ criteria.js           â† Scoring criteria
â”œâ”€â”€ output/                   â† Generated reports
â”‚   â”œâ”€â”€ test-results-*.json
â”‚   â””â”€â”€ test-report-*.md
â””â”€â”€ fixtures/                 â† Test data files
    â”œâ”€â”€ sample.py
    â”œâ”€â”€ content.md
    â””â”€â”€ ...
```

## Next Steps

After running tests:

1. **Review reports** in `tests/output/`
2. **Check failed tests** - see detailed feedback
3. **Iterate on issues** - fix bugs or update tests
4. **Update scenarios** - add new test cases as needed
5. **CI/CD integration** - add to your pipeline

## Tips

- Run comprehensive tests before releases
- Keep scenarios updated with new features
- Review judge feedback for quality insights
- Use pass rate trends to track progress
- Failed tests often reveal UX issues
